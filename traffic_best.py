# -*- coding: utf-8 -*-
"""traffic_best.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GdNTojDJekHout6V2oQeaYhSI5rk1ckE
"""

# Cell 1: Install required libraries
#!pip install osmnx networkx numpy matplotlib scipy



# Cell 2: Import libraries and set up environment
import osmnx as ox
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
import random
import time
import heapq
from collections import deque, defaultdict
import math
from scipy.spatial import cKDTree

# Set up environment settings
ox.settings.log_console = True
ox.settings.use_cache = False
print("Libraries imported and environment configured.")

# Cell 3: Extract road network from OpenStreetMap
def extract_road_network(place_name="Manhattan, New York City, USA", network_type="drive"):
    """Extract road network from OpenStreetMap by place name"""
    print(f"Extracting road network for {place_name}...")
    graph = ox.graph_from_place(place_name, network_type=network_type)

    # Project to UTM for accurate distance calculations
    graph = ox.project_graph(graph)

    # Convert to undirected graph for community detection
    undirected_graph = nx.Graph(graph)

    # Extract node coordinates for spatial calculations
    for node, data in undirected_graph.nodes(data=True):
        undirected_graph.nodes[node]['x'] = data.get('x', 0)
        undirected_graph.nodes[node]['y'] = data.get('y', 0)

    print(f"Network extracted with {len(undirected_graph.nodes)} nodes and {len(undirected_graph.edges)} edges")
    return undirected_graph

# Extract a sample road network
G = extract_road_network()

# Cell 4: Prepare graph by calculating influence and gravity
def calculate_node_influence(G):
    """Calculate traffic influence of each junction"""
    for node in G.nodes():
        # Base influence from degree centrality
        connectivity = G.degree(node)

        # Simulate traffic with temporal variation
        traffic_density = np.random.randint(50, 150)

        # Calculate influence as in paper
        influence = connectivity * traffic_density / 100

        # Store influence value
        G.nodes[node]['influence'] = influence

    return G

def calculate_gravity(G):
    """Calculate gravity between connected nodes using formula g(A,B) = i(A)*i(B)/d(A,B)"""
    for u, v, data in G.edges(data=True):
        influence_u = G.nodes[u]['influence']
        influence_v = G.nodes[v]['influence']

        # Get distance between nodes
        distance = data.get('length', 1)
        if distance == 0:
            distance = 1

        # Calculate gravity
        gravity = (influence_u * influence_v) / distance

        # Add path continuity factor to improve path quality
        path_factor = 1.0
        if abs(G.degree(u) - G.degree(v)) <= 1:
            path_factor = 1.5  # Boost gravity for nodes with similar connectivity

        gravity *= path_factor

        # Store gravity and routing weight
        G[u][v]['gravity'] = gravity
        G[u][v]['weight'] = distance * (1 + data.get('traffic_density', 50)/500)

    return G

def prepare_graph(G):
    """Prepare graph with all necessary attributes"""
    # Add traffic density to edges (simulated)
    for u, v, data in G.edges(data=True):
        G[u][v]['traffic_density'] = np.random.randint(1, 100)
        if 'length' not in data:
            G[u][v]['length'] = 1

    G = calculate_node_influence(G)
    G = calculate_gravity(G)
    print("Graph prepared with influence and gravity values")
    return G

# Prepare the graph
G = prepare_graph(G)

# Cell 5: Gravity-based community detection (keeping original approach)
def gravity_based_cluster_detection(G, min_size=20, max_size=100):
    """
    Enhanced gravity-based cluster detection with adaptive sizing

    Args:
        G: NetworkX graph
        min_size: Minimum cluster size
        max_size: Maximum cluster size

    Returns:
        Dictionary mapping nodes to cluster IDs
    """
    # Initialize data structures
    unassigned = list(G.nodes())
    clusters = {}
    cluster_id = 1

    # Calculate network density for adaptive sizing
    network_density = len(G.edges()) / len(G.nodes())

    while unassigned:
        # Choose seed with highest influence/connectivity
        seed = max(unassigned, key=lambda node: G.nodes[node].get('influence', 0))

        # Adaptive community size based on local density
        local_density = len(list(G.neighbors(seed))) / network_density
        # Smaller communities in denser areas for better path quality
        adaptive_size = int(max_size - (local_density * (max_size - min_size) / 2))
        adaptive_size = max(min_size, min(max_size, adaptive_size))

        # Initialize BFS queue with seed
        queue = deque([seed])
        clusters[seed] = cluster_id
        unassigned.remove(seed)
        current_size = 1

        # Grow community based on gravity
        while queue and current_size < adaptive_size:
            current = queue.popleft()

            # Create max heap for neighbors based on gravity
            neighbor_heap = []
            for neighbor in G.neighbors(current):
                if neighbor in unassigned:
                    gravity = G[current][neighbor]['gravity']
                    heapq.heappush(neighbor_heap, (-gravity, neighbor))

            # Add highest gravity neighbors to cluster
            while neighbor_heap and current_size < adaptive_size:
                _, node = heapq.heappop(neighbor_heap)
                if node in unassigned:
                    clusters[node] = cluster_id
                    unassigned.remove(node)
                    queue.append(node)
                    current_size += 1

        cluster_id += 1

    print(f"Created {cluster_id-1} communities using gravity-based detection")
    return clusters

# Apply community detection
clusters = gravity_based_cluster_detection(G)

# Cell 6: Identify transit nodes within communities
def identify_transit_nodes(G, clusters, transit_fraction=0.15):
    """
    Identify key transit nodes within each community

    Args:
        G: NetworkX graph
        clusters: Dictionary mapping nodes to cluster IDs
        transit_fraction: Fraction of nodes to select as transit nodes

    Returns:
        Dictionary mapping community IDs to their transit nodes
    """
    # Get reverse mapping (community to nodes)
    community_nodes = defaultdict(list)
    for node, comm_id in clusters.items():
        community_nodes[comm_id].append(node)

    # Identify transit nodes for each community
    transit_nodes = {}

    for comm_id, nodes in community_nodes.items():
        # Calculate betweenness centrality within community
        subgraph = G.subgraph(nodes)

        # For large communities, use approximate betweenness for efficiency
        if len(nodes) > 500:
            betweenness = nx.approximate_current_flow_betweenness_centrality(
                subgraph, normalized=True, epsilon=0.1
            )
        else:
            betweenness = nx.betweenness_centrality(subgraph, normalized=True)

        # Select top nodes as transit nodes
        num_transit = max(3, int(len(nodes) * transit_fraction))
        top_nodes = sorted(betweenness.keys(), key=lambda n: betweenness[n], reverse=True)[:num_transit]

        # Add border nodes (nodes connected to other communities)
        border_nodes = []
        for node in nodes:
            for neighbor in G.neighbors(node):
                if neighbor in clusters and clusters[neighbor] != comm_id:
                    border_nodes.append(node)
                    break

        # Combine top central nodes with border nodes
        community_transit = set(top_nodes + border_nodes)
        transit_nodes[comm_id] = list(community_transit)

    # Count total transit nodes
    total_transit = sum(len(nodes) for nodes in transit_nodes.values())
    print(f"Identified {total_transit} transit nodes across {len(transit_nodes)} communities")

    return transit_nodes

# Identify transit nodes
transit_nodes = identify_transit_nodes(G, clusters)

# Cell 7: Precompute optimal paths between transit nodes
def precompute_transit_paths(G, transit_nodes):
    """
    Precompute shortest paths between transit nodes

    Args:
        G: NetworkX graph
        transit_nodes: Dictionary mapping community IDs to transit nodes

    Returns:
        Dictionary with precomputed paths and distances
    """
    # Flatten all transit nodes into a single list
    all_transit = []
    for nodes in transit_nodes.values():
        all_transit.extend(nodes)

    # Remove duplicates
    all_transit = list(set(all_transit))

    # Precompute all-pairs shortest paths between transit nodes
    print(f"Precomputing paths between {len(all_transit)} transit nodes...")

    # Use Floyd-Warshall for all-pairs shortest paths
    transit_graph = G.subgraph(all_transit)

    # Calculate shortest paths and distances
    start_time = time.time()
    transit_distances = dict(nx.all_pairs_dijkstra_path_length(transit_graph, weight='weight'))
    transit_paths = dict(nx.all_pairs_dijkstra_path(transit_graph, weight='weight'))

    print(f"Precomputation completed in {time.time() - start_time:.2f} seconds")

    return {
        'distances': transit_distances,
        'paths': transit_paths
    }

# Precompute paths between transit nodes
transit_data = precompute_transit_paths(G, transit_nodes)

# Cell 8: Implement Transit-Node Enhanced pathfinding
def find_nearest_transit_nodes(G, node, transit_nodes, clusters, k=3):
    """Find k nearest transit nodes to given node within its community"""
    community = clusters.get(node)
    if community is None:
        return []

    # Get transit nodes in this community
    community_transit = transit_nodes.get(community, [])

    if not community_transit:
        return []

    # Calculate distances to all transit nodes in community
    distances = {}
    for transit in community_transit:
        try:
            dist = nx.dijkstra_path_length(G, node, transit, weight='weight')
            distances[transit] = dist
        except nx.NetworkXNoPath:
            continue

    # Sort by distance and take top k
    nearest = sorted(distances.keys(), key=lambda n: distances[n])[:k]
    return [(n, distances[n]) for n in nearest]

def refine_path(G, path):
    """Smooth path by removing unnecessary detours"""
    if len(path) < 3:
        return path

    refined = [path[0]]
    i = 0

    while i < len(path) - 2:
        # Check if we can skip the next node
        if G.has_edge(path[i], path[i+2]):
            # Compare weights to ensure the shortcut is beneficial
            direct = G[path[i]][path[i+2]]['weight']
            indirect = G[path[i]][path[i+1]]['weight'] + G[path[i+1]][path[i+2]]['weight']

            if direct <= indirect * 1.05:  # Allow 5% tolerance
                i += 2  # Skip the middle node
                refined.append(path[i])
            else:
                i += 1
                refined.append(path[i])
        else:
            i += 1
            refined.append(path[i])

    # Ensure the final node is included
    if refined[-1] != path[-1]:
        refined.append(path[-1])

    return refined

def transit_node_routing(G, transit_nodes, transit_data, clusters, source, target):
    """
    Transit-Node Enhanced Community Routing

    Args:
        G: NetworkX graph
        transit_nodes: Dictionary mapping community IDs to transit nodes
        transit_data: Precomputed paths and distances
        clusters: Dictionary mapping nodes to cluster IDs
        source: Source node
        target: Target node

    Returns:
        Path and its length
    """
    # Special case: source and target in same community
    source_community = clusters.get(source)
    target_community = clusters.get(target)

    if source_community is None or target_community is None:
        return None, float('inf')

    if source_community == target_community:
        try:
            # Direct path within community
            path = nx.shortest_path(G, source, target, weight='weight')
            length = sum(G[path[i]][path[i+1]]['weight'] for i in range(len(path)-1))
            return path, length
        except nx.NetworkXNoPath:
            return None, float('inf')

    # Find nearest transit nodes to source and target
    source_transit = find_nearest_transit_nodes(G, source, transit_nodes, clusters)
    target_transit = find_nearest_transit_nodes(G, target, transit_nodes, clusters)

    if not source_transit or not target_transit:
        # Fall back to Dijkstra if transit nodes not found
        try:
            path = nx.shortest_path(G, source, target, weight='weight')
            length = sum(G[path[i]][path[i+1]]['weight'] for i in range(len(path)-1))
            return path, length
        except nx.NetworkXNoPath:
            return None, float('inf')

    # Find best path through transit nodes
    best_path = None
    best_length = float('inf')

    for src_transit, src_dist in source_transit:
        for tgt_transit, tgt_dist in target_transit:
            if src_transit == tgt_transit:
                # Source and target connect to same transit node
                try:
                    src_to_transit = nx.shortest_path(G, source, src_transit, weight='weight')
                    transit_to_tgt = nx.shortest_path(G, src_transit, target, weight='weight')

                    # Combine paths (remove duplicate transit node)
                    path = src_to_transit + transit_to_tgt[1:]

                    # Calculate length
                    length = src_dist + tgt_dist

                    if length < best_length:
                        best_length = length
                        best_path = path
                except nx.NetworkXNoPath:
                    continue
            else:
                # Check if transit-to-transit path exists in precomputed data
                if (src_transit in transit_data['distances'] and
                    tgt_transit in transit_data['distances'][src_transit]):

                    transit_dist = transit_data['distances'][src_transit][tgt_transit]
                    transit_path = transit_data['paths'][src_transit][tgt_transit]

                    # Calculate total path and length
                    try:
                        src_to_transit = nx.shortest_path(G, source, src_transit, weight='weight')
                        transit_to_tgt = nx.shortest_path(G, tgt_transit, target, weight='weight')

                        # Combine paths
                        path = src_to_transit[:-1] + transit_path + transit_to_tgt[1:]
                        length = src_dist + transit_dist + tgt_dist

                        if length < best_length:
                            best_length = length
                            best_path = path
                    except nx.NetworkXNoPath:
                        continue

    # If a path was found, refine it
    if best_path:
        refined_path = refine_path(G, best_path)
        # Recalculate length after refinement
        refined_length = sum(G[refined_path[i]][refined_path[i+1]]['weight']
                            for i in range(len(refined_path)-1))
        return refined_path, refined_length

    # Fall back to standard Dijkstra if no path found through transit nodes
    try:
        path = nx.shortest_path(G, source, target, weight='weight')
        length = sum(G[path[i]][path[i+1]]['weight'] for i in range(len(path)-1))
        return path, length
    except nx.NetworkXNoPath:
        return None, float('inf')

# Cell 9: Benchmark the TNECR algorithm
def benchmark_tnecr_algorithm(G, transit_nodes, transit_data, clusters, num_trials=50):
    """
    Benchmark the Transit-Node Enhanced Community Routing algorithm

    Args:
        G: NetworkX graph
        transit_nodes: Dictionary mapping community IDs to transit nodes
        transit_data: Precomputed paths and distances
        clusters: Dictionary mapping nodes to cluster IDs
        num_trials: Number of random trials

    Returns:
        Dictionary of benchmarking results
    """
    nodes = list(G.nodes())

    # For storing results
    dijkstra_times = []
    dijkstra_lengths = []
    tnecr_times = []
    tnecr_lengths = []

    print(f"Running {num_trials} benchmark trials...")
    for i in range(num_trials):
        # Select random source and target nodes
        source, target = random.sample(nodes, 2)

        # Run standard Dijkstra
        start_time = time.time()
        try:
            std_path = nx.shortest_path(G, source, target, weight='weight')
            std_length = sum(G[std_path[i]][std_path[i+1]]['weight'] for i in range(len(std_path)-1))
            dijkstra_lengths.append(std_length)
        except nx.NetworkXNoPath:
            std_length = float('inf')
        dijkstra_time = time.time() - start_time
        dijkstra_times.append(dijkstra_time)

        # Run TNECR algorithm
        start_time = time.time()
        tnecr_path, tnecr_length = transit_node_routing(
            G, transit_nodes, transit_data, clusters, source, target
        )
        tnecr_time = time.time() - start_time

        if tnecr_path is not None:
            tnecr_lengths.append(tnecr_length)

        tnecr_times.append(tnecr_time)

        if (i+1) % 10 == 0:
            print(f"Completed {i+1}/{num_trials} trials")

    # Calculate statistics
    avg_dijkstra_time = sum(dijkstra_times) / len(dijkstra_times)
    avg_tnecr_time = sum(tnecr_times) / len(tnecr_times)
    time_saved = avg_dijkstra_time - avg_tnecr_time
    time_saved_percent = (time_saved / avg_dijkstra_time) * 100

    # Calculate average path lengths (for valid pairs)
    valid_pairs = [(d, t) for d, t in zip(dijkstra_lengths, tnecr_lengths)
                   if d != float('inf') and t != float('inf')]

    if valid_pairs:
        dijkstra_valid, tnecr_valid = zip(*valid_pairs)
        avg_dijkstra_length = sum(dijkstra_valid) / len(dijkstra_valid)
        avg_tnecr_length = sum(tnecr_valid) / len(tnecr_valid)
        avg_length_diff = avg_tnecr_length - avg_dijkstra_length
        length_diff_percent = (avg_length_diff / avg_dijkstra_length) * 100
    else:
        avg_dijkstra_length = 0
        avg_tnecr_length = 0
        avg_length_diff = 0
        length_diff_percent = 0

    # Print results
    print("\n--- Benchmark Results ---")
    print(f"Original Graph: {len(G.nodes())} nodes, {len(G.edges())} edges")

    print(f"\nTime Performance:")
    print(f"Standard Dijkstra: {avg_dijkstra_time:.6f} s")
    print(f"TNECR Algorithm: {avg_tnecr_time:.6f} s")
    print(f"Time Saved: {time_saved:.6f} s ({time_saved_percent:.2f}%)")

    print(f"\nPath Quality:")
    print(f"Standard Dijkstra Avg Length: {avg_dijkstra_length:.2f}")
    print(f"TNECR Algorithm Avg Length: {avg_tnecr_length:.2f}")
    print(f"Average Length Difference: {avg_length_diff:.2f}")
    print(f"Distance Loss: {length_diff_percent:.2f}%")

    return {
        "dijkstra_time": avg_dijkstra_time,
        "tnecr_time": avg_tnecr_time,
        "time_saved_percent": time_saved_percent,
        "distance_loss_percent": length_diff_percent
    }

# Run benchmark
benchmark_results = benchmark_tnecr_algorithm(G, transit_nodes, transit_data, clusters)